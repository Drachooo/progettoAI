{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f35e",
   "metadata": {},
   "source": [
    "# AI Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233976dd",
   "metadata": {},
   "source": [
    "An advanced aquatic drone, is deployed to collect critical data on marine biodiversity in a coastal region. The drone starts at point $S$, located near the shore, and must navigate to point $G$, a designated marine research site rich in coral reefs and sea life. Along the way, the drone must carefully maneuver through dynamic underwater environments, avoiding hazards and optimizing its energy usage.\n",
    "\n",
    "The environment includes:  \n",
    "1. **(O) Open Water:** Normal movement; no additional challenges.  \n",
    "2. **(C) Currents:** Areas where the drone's movement is influenced by ocean currents, potentially pushing it off course.  \n",
    "3. **(F) Seaweed Forests:** Dense vegetation that slows down the drone, incurring extra energy costs per move.    \n",
    "5. **(E) Energy Stations:** Specific points where the drone can recharge its battery, reducing the total cost navigation.  \n",
    "\n",
    "\n",
    "<img src=\"images/env_ex2.png\" style=\"zoom: 20%;\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Environment Details:\n",
    "\n",
    "- **Grid Representation:** The environment is represented as in the above image (a grid 10x10).  \n",
    "- $S$ - Start state: The drone's starting point at (0, 0).  \n",
    "- $G$ - Goal state: The marine research site at (9, 7), providing a large positive reward +20.0 and ending the episode.  \n",
    "- **Movement Costs:** Each move has a default energy cost of -0.04.  \n",
    "- **Hazards:**  \n",
    "  - **Strong Currents:** Entering a zone with current results in a stochastic movement:  \n",
    "    - 80% chance to move as intended.  \n",
    "    - 10% chance to be pushed one cell in the left direction perpendicular to the desired movement.  \n",
    "    - 10% chance to be pushed one cell in the right direction perpendicular to the desired movement. \n",
    "  - **Seaweed Forests:** Entering these zones incurs an additional -0.2 reward penalty with respect to the standard movement cost (i.e., a total -0.24 penalty).\n",
    "\n",
    "- **Energy Stations:** Provide a +1.0 reward when visited (however reaching these cells may require the agent to move far from the goal).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92438f5",
   "metadata": {},
   "source": [
    "You can use the following code to explore better the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be1e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'O' 'O' 'F' 'F' 'F' 'F' 'O' 'O' 'O']\n",
      " ['O' 'F' 'C' 'C' 'C' 'O' 'F' 'E' 'F' 'O']\n",
      " ['O' 'O' 'F' 'F' 'F' 'O' 'F' 'F' 'F' 'C']\n",
      " ['F' 'C' 'F' 'F' 'E' 'C' 'F' 'O' 'F' 'C']\n",
      " ['F' 'C' 'F' 'F' 'F' 'C' 'F' 'O' 'F' 'C']\n",
      " ['F' 'E' 'F' 'O' 'O' 'O' 'F' 'E' 'F' 'C']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'F' 'F' 'F' 'C']\n",
      " ['O' 'F' 'F' 'F' 'O' 'O' 'O' 'F' 'F' 'C']\n",
      " ['O' 'O' 'O' 'O' 'F' 'F' 'F' 'F' 'F' 'C']\n",
      " ['F' 'F' 'F' 'O' 'O' 'O' 'O' 'G' 'O' 'F']]\n",
      "\n",
      "Actions encoding:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Cell type of start state:  S\n",
      "Cell type of goal state:  G\n",
      "Cell type of cell (0, 3):  F\n",
      "Cell type of cell (1, 2):  C\n",
      "Cell type of cell (1, 7):  E\n"
     ]
    }
   ],
   "source": [
    "import os, sys \n",
    "import tqdm\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "env_name = 'AquaticEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(\"\\nActions encoding: \", env.actions)\n",
    "\n",
    "# Remember that you can know the type of a cell whenever you need by accessing the grid element of the environment:\n",
    "print(\"Cell type of start state: \",env.grid[env.startstate])\n",
    "print(\"Cell type of goal state: \",env.grid[env.goalstate])\n",
    "state = 3 # forest\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "state = 12 # corrent\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "state = 17 # energy station\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "caecc567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actions encoding:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Cell type of start state:  S\n",
      "Cell type of goal state: G Reward: 20.0\n",
      "\n",
      "Cell type of cell (0, 1):  O\n",
      "Probability of effectivelty performing action L from cell (0, 1) to cell (0, 2): 0.0\n",
      "Probability of effectivelty performing action R from cell (0, 1) to cell (0, 2): 1.0\n",
      "Probability of effectivelty performing action U from cell (0, 1) to cell (0, 2): 0.0\n",
      "Probability of effectivelty performing action D from cell (0, 1) to cell (0, 2): 0.0\n",
      "\n",
      "Cell type of cell (1, 3):  C\n",
      "Probability of effectivelty performing action L from cell (1, 3) to cell (1, 4): 0.0\n",
      "Probability of effectivelty performing action R from cell (1, 3) to cell (1, 4): 0.8\n",
      "Probability of effectivelty performing action U from cell (1, 3) to cell (1, 4): 0.1\n",
      "Probability of effectivelty performing action D from cell (1, 3) to cell (1, 4): 0.1\n"
     ]
    }
   ],
   "source": [
    "#Action encoding\n",
    "print(\"\\nActions encoding: \", env.actions)\n",
    "\n",
    "# Remember that you can know the type of a cell whenever you need by accessing the grid element of the environment:\n",
    "print(\"Cell type of start state: \",env.grid[env.startstate])\n",
    "print(\"Cell type of goal state: {} Reward: {}\".format(env.grid[env.goalstate],env.RS[env.goalstate]))\n",
    "\n",
    "state = 1 # normal state\n",
    "print(f\"\\nCell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "print(f\"Probability of effectivelty performing action {env.actions[0]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[0], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[1]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[1], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[2]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[2], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[3]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[3], state+1]}\")\n",
    "\n",
    "state = 13 # state with stochastic transitions\n",
    "print(f\"\\nCell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "print(f\"Probability of effectivelty performing action {env.actions[0]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[0], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[1]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[1], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[2]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[2], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[3]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[3], state+1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f146690",
   "metadata": {},
   "source": [
    "#### Q1. Find an optimal solution to this problem by using the approach that you think is most appropriate. Motivate your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dadd6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'AquaticEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "def acquaticValueIteration(env, maxIterazione = 500, discount = 0.9, maxDifferenza = 1e-3):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: ambiente\n",
    "        maxIterazione: numero massimo di iterazioni\n",
    "        discount: \n",
    "        maxDifferenza: limite superiore da confrontare con delta\n",
    "\n",
    "    Returns:\n",
    "        policy: vettore contenente le azioni contentute nel rispettivo stato identificato da \"i\"\n",
    "    \"\"\"\n",
    "\n",
    "    U_1 = [0 for _ in range(env.observation_space.n)] #array contenente i valori degli stati aggiornati\n",
    "    U = U_1.copy()  #array contenente i valori dei vecchi stati\n",
    "    cont = 0\n",
    "\n",
    "    while True:\n",
    "        cont += 1\n",
    "        delta = 0\n",
    "        U = U_1.copy()\n",
    "\n",
    "        for state in range(0, env.observation_space.n):\n",
    "            if env.grid[state] == \"G\":\n",
    "                U_1[state] = env.RS[state]\n",
    "            else:\n",
    "                insiemePunteggiAzioni = [0 for _ in range(0, env.action_space.n)]\n",
    "                \n",
    "                for action in range(0, env.action_space.n):\n",
    "                    for nextState in range (0, env.observation_space.n):\n",
    "                        insiemePunteggiAzioni[action] += env.T[state, action, nextState] * U[nextState]\n",
    "                    \n",
    "                U_1[state] = env.RS[state] + discount * max(insiemePunteggiAzioni)\n",
    "            \n",
    "            delta = max(delta, abs(U_1[state] - U[state]))\n",
    "        \n",
    "        if delta < maxDifferenza * (1 - discount) / discount or cont >= maxIterazione:\n",
    "            break\n",
    "\n",
    "    return(values_to_policy(np.asarray(U), env), U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8fc80dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'AquaticEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "def acquaticPolicyIteration(env, maxIterazione = 500, discount = 0.9, maxDifferenza = 1e-3):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: ambiente\n",
    "        maxIterazione: numero massimo di iterazioni\n",
    "        discount: \n",
    "        maxDifferenza: limite superiore da confrontare con delta\n",
    "        \n",
    "    Returns:\n",
    "        policy: vettore contenente le azioni contentute nel rispettivo stato identificato da \"i\"\n",
    "    \"\"\"\n",
    "\n",
    "    policy = [0 for _ in range(env.observation_space.n)]\n",
    "    \n",
    "    U_1 = [0 for _ in range(env.observation_space.n)] #array contenente i valori degli stati aggiornati\n",
    "    U = U_1.copy()  #array contenente i valori dei vecchi stati\n",
    "    def argmax(valore):\n",
    "        return max(range(len(valore)), key = lambda x: valore[x])\n",
    "\n",
    "    cont = 0\n",
    "    while True:\n",
    "        contIterazione = 0\n",
    "        while True:\n",
    "            contIterazione += 1\n",
    "            delta = 0\n",
    "            U = U_1.copy()\n",
    "\n",
    "            for state in range(0, env.observation_space.n):\n",
    "                if env.grid[state] == \"G\":\n",
    "                    U_1[state] = env.RS[state]\n",
    "                else:\n",
    "                    u_somma = 0\n",
    "                    \n",
    "                    for nextState in range (0, env.observation_space.n):\n",
    "                        u_somma += env.T[state, policy[state], nextState] * U[nextState]\n",
    "                    \n",
    "                    U_1[state] = env.RS[state] + discount * u_somma\n",
    "                \n",
    "                delta = max(delta, abs(U_1[state] - U[state]))\n",
    "            \n",
    "            if delta < maxDifferenza * (1 - discount) / discount or contIterazione >= maxIterazione:\n",
    "                break\n",
    "        \n",
    "        U = U_1.copy()\n",
    "        unchanged = True\n",
    "        cont += 1\n",
    "\n",
    "        for state in range(0, env.observation_space.n):\n",
    "            insiemePunteggiAzioni = [0 for _ in range(0, env.action_space.n)]\n",
    "        \n",
    "            for action in range(0,env.action_space.n):\n",
    "                for nextState in range(0, env.observation_space.n):\n",
    "                    insiemePunteggiAzioni[action] += env.T[state, action, nextState] * U[nextState]\n",
    "            \n",
    "            sommaPolicy = 0\n",
    "            for nextState in range(0, env.observation_space.n):\n",
    "                sommaPolicy += env.T[state, policy[state], nextState] * U[nextState]\n",
    "\n",
    "            if max(insiemePunteggiAzioni) > sommaPolicy:\n",
    "                policy[state] = argmax(insiemePunteggiAzioni)\n",
    "                unchanged = False\n",
    "\n",
    "        if unchanged or cont >= maxIterazione:\n",
    "            break\n",
    "    \n",
    "    return(values_to_policy(np.asarray(U), env), U)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a45fd6",
   "metadata": {},
   "source": [
    "You can visualize and check your solution using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5baf89b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metodo: Value Iteration\n",
      "[['D' 'D' 'D' 'D' 'R' 'D' 'R' 'D' 'L' 'L']\n",
      " ['D' 'D' 'R' 'R' 'R' 'R' 'R' 'D' 'L' 'L']\n",
      " ['R' 'D' 'R' 'R' 'D' 'R' 'R' 'D' 'L' 'L']\n",
      " ['D' 'D' 'R' 'R' 'D' 'R' 'R' 'D' 'L' 'L']\n",
      " ['D' 'D' 'R' 'D' 'D' 'D' 'R' 'D' 'L' 'L']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'D' 'L' 'L']\n",
      " ['D' 'R' 'R' 'R' 'R' 'D' 'D' 'D' 'D' 'D']\n",
      " ['D' 'D' 'D' 'D' 'R' 'R' 'D' 'D' 'D' 'D']\n",
      " ['R' 'R' 'R' 'D' 'D' 'D' 'D' 'D' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'L']]\n",
      "\n",
      "Tempo di esecuzione: \n",
      "0.709\n",
      "\n",
      "Metodo: Policy Iteration\n",
      "[['D' 'D' 'D' 'D' 'R' 'D' 'R' 'D' 'L' 'L']\n",
      " ['D' 'D' 'R' 'R' 'R' 'R' 'R' 'D' 'L' 'L']\n",
      " ['R' 'D' 'R' 'R' 'D' 'R' 'R' 'D' 'L' 'L']\n",
      " ['D' 'D' 'R' 'R' 'D' 'R' 'R' 'D' 'L' 'L']\n",
      " ['D' 'D' 'R' 'D' 'D' 'D' 'R' 'D' 'L' 'L']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'D' 'L' 'L']\n",
      " ['D' 'R' 'R' 'R' 'R' 'D' 'D' 'D' 'D' 'D']\n",
      " ['D' 'D' 'D' 'D' 'R' 'R' 'D' 'D' 'D' 'D']\n",
      " ['R' 'R' 'R' 'D' 'D' 'D' 'D' 'D' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'L']]\n",
      "\n",
      "Tempo di esecuzione: \n",
      "2.6671\n"
     ]
    }
   ],
   "source": [
    "print(\"Metodo: Value Iteration\")\n",
    "\n",
    "t = timer()\n",
    "solution, _ = acquaticValueIteration(env)\n",
    "visual_solution = np.vectorize(env.actions.get)(solution.reshape(env.rows, env.cols)) \n",
    "\n",
    "print(visual_solution)\n",
    "print(\"\\nTempo di esecuzione: \\n{}\".format(round(timer() - t, 4)))\n",
    "\n",
    "print(\"\\nMetodo: Policy Iteration\")\n",
    "\n",
    "t1 = timer()\n",
    "solution, _ = acquaticPolicyIteration(env)\n",
    "visual_solution = np.vectorize(env.actions.get)(solution.reshape(env.rows, env.cols)) \n",
    "\n",
    "print(visual_solution)\n",
    "print(\"\\nTempo di esecuzione: \\n{}\".format(round(timer() - t1, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e812b",
   "metadata": {},
   "source": [
    "Confronto tra gli algoritmi implementati:\n",
    "\n",
    "Il *Value Iteration* converge alla funzione valore ottimale V∗ solo in modo asintotico: ci si ferma quando il cambiamento tra due iterazioni è inferiore alla soglia maxDifferenza, tuttavia non si ha mai la certezza matematica assoluta di aver raggiunto il valore esatto, ma solo un'approssimazione molto stretta.\n",
    "\n",
    "Il *Policy Iteration* converge alla policy ottimale in un numero finito di iterazioni. La Policy Iteration esplora sistematicamente le opzioni fino a trovare quella perfetta.\n",
    "Il calcolo costa però al computer maggior tempo e maggior spazio, in correlazione a quanto grande è il problema da affrontare o l'ambiente preso in considerazione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e100a",
   "metadata": {},
   "source": [
    "#### Analyze the solution returned by your approach and comment on whether the solution passes by at least two charging stations to reach the coral for every possible execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc75a66a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0aa199c5",
   "metadata": {},
   "source": [
    "Nella Value Iteration, data la stocasticità della transizione nelle Correnti, solo nel caso la sonda si trovi nella cella (4,1)  vi è un 10% di probabilità utilizzando la action migliore, ovvero DOWN, di essere spinto verso RIGHT, intraprendendo un percorso che non prevede il passaggio in nessuna seconda colonna di Energia. Di conseguenza non esiste nessuna certezza di passare in due colonne per ogni esecuzione del programma.\n",
    "\n",
    "Analogamente, nella Policy Iteration, avendo trovao la medesima soluzione del Value Iteration, vi sarà la stessa probabilità di essere spinti via dalla corrente e di intraprendere un percorso che non prevede il passaggio per due colonne di Energia.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
