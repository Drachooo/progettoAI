{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f35e",
   "metadata": {},
   "source": [
    "# AI Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233976dd",
   "metadata": {},
   "source": [
    "An advanced aquatic drone, is deployed to collect critical data on marine biodiversity in a coastal region. The drone starts at point $S$, located near the shore, and must navigate to point $G$, a designated marine research site rich in coral reefs and sea life. Along the way, the drone must carefully maneuver through dynamic underwater environments, avoiding hazards and optimizing its energy usage.\n",
    "\n",
    "The environment includes:  \n",
    "1. **(O) Open Water:** Normal movement; no additional challenges.  \n",
    "2. **(C) Currents:** Areas where the drone's movement is influenced by ocean currents, potentially pushing it off course.  \n",
    "3. **(F) Seaweed Forests:** Dense vegetation that slows down the drone, incurring extra energy costs per move.    \n",
    "5. **(E) Energy Stations:** Specific points where the drone can recharge its battery, reducing the total cost navigation.  \n",
    "\n",
    "\n",
    "<img src=\"images/env_ex2.png\" style=\"zoom: 20%;\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Environment Details:\n",
    "\n",
    "- **Grid Representation:** The environment is represented as in the above image (a grid 10x10).  \n",
    "- $S$ - Start state: The drone's starting point at (0, 0).  \n",
    "- $G$ - Goal state: The marine research site at (9, 7), providing a large positive reward +20.0 and ending the episode.  \n",
    "- **Movement Costs:** Each move has a default energy cost of -0.04.  \n",
    "- **Hazards:**  \n",
    "  - **Strong Currents:** Entering a zone with current results in a stochastic movement:  \n",
    "    - 80% chance to move as intended.  \n",
    "    - 10% chance to be pushed one cell in the left direction perpendicular to the desired movement.  \n",
    "    - 10% chance to be pushed one cell in the right direction perpendicular to the desired movement. \n",
    "  - **Seaweed Forests:** Entering these zones incurs an additional -0.2 reward penalty with respect to the standard movement cost (i.e., a total -0.24 penalty).\n",
    "\n",
    "- **Energy Stations:** Provide a +1.0 reward when visited (however reaching these cells may require the agent to move far from the goal).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92438f5",
   "metadata": {},
   "source": [
    "You can use the following code to explore better the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be1e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'O' 'O' 'F' 'F' 'F' 'F' 'O' 'O' 'O']\n",
      " ['O' 'F' 'C' 'C' 'C' 'O' 'F' 'E' 'F' 'O']\n",
      " ['O' 'O' 'F' 'F' 'F' 'O' 'F' 'F' 'F' 'C']\n",
      " ['F' 'C' 'F' 'F' 'E' 'C' 'F' 'O' 'F' 'C']\n",
      " ['F' 'C' 'F' 'F' 'F' 'C' 'F' 'O' 'F' 'C']\n",
      " ['F' 'E' 'F' 'O' 'O' 'O' 'F' 'E' 'F' 'C']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'F' 'F' 'F' 'C']\n",
      " ['O' 'F' 'F' 'F' 'O' 'O' 'O' 'F' 'F' 'C']\n",
      " ['O' 'O' 'O' 'O' 'F' 'F' 'F' 'F' 'F' 'C']\n",
      " ['F' 'F' 'F' 'O' 'O' 'O' 'O' 'G' 'O' 'F']]\n",
      "\n",
      "Actions encoding:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Cell type of start state:  S\n",
      "Cell type of goal state:  G\n",
      "Cell type of cell (0, 3):  F\n",
      "Cell type of cell (1, 2):  C\n",
      "Cell type of cell (1, 7):  E\n"
     ]
    }
   ],
   "source": [
    "import os, sys \n",
    "import tqdm\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "env_name = 'AquaticEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(\"\\nActions encoding: \", env.actions)\n",
    "\n",
    "# Remember that you can know the type of a cell whenever you need by accessing the grid element of the environment:\n",
    "print(\"Cell type of start state: \",env.grid[env.startstate])\n",
    "print(\"Cell type of goal state: \",env.grid[env.goalstate])\n",
    "state = 3 # forest\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "state = 12 # corrent\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "state = 17 # energy station\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caecc567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actions encoding:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Cell type of start state:  S\n",
      "Cell type of goal state: G Reward: 20.0\n",
      "\n",
      "Cell type of cell (0, 1):  O\n",
      "Probability of effectivelty performing action L from cell (0, 1) to cell (0, 2): 0.0\n",
      "Probability of effectivelty performing action R from cell (0, 1) to cell (0, 2): 1.0\n",
      "Probability of effectivelty performing action U from cell (0, 1) to cell (0, 2): 0.0\n",
      "Probability of effectivelty performing action D from cell (0, 1) to cell (0, 2): 0.0\n",
      "\n",
      "Cell type of cell (1, 3):  C\n",
      "Probability of effectivelty performing action L from cell (1, 3) to cell (1, 4): 0.0\n",
      "Probability of effectivelty performing action R from cell (1, 3) to cell (1, 4): 0.8\n",
      "Probability of effectivelty performing action U from cell (1, 3) to cell (1, 4): 0.1\n",
      "Probability of effectivelty performing action D from cell (1, 3) to cell (1, 4): 0.1\n"
     ]
    }
   ],
   "source": [
    "#Action encoding\n",
    "print(\"\\nActions encoding: \", env.actions)\n",
    "\n",
    "# Remember that you can know the type of a cell whenever you need by accessing the grid element of the environment:\n",
    "print(\"Cell type of start state: \",env.grid[env.startstate])\n",
    "print(\"Cell type of goal state: {} Reward: {}\".format(env.grid[env.goalstate],env.RS[env.goalstate]))\n",
    "\n",
    "state = 1 # normal state\n",
    "print(f\"\\nCell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "print(f\"Probability of effectivelty performing action {env.actions[0]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[0], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[1]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[1], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[2]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[2], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[3]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[3], state+1]}\")\n",
    "\n",
    "state = 13 # state with stochastic transitions\n",
    "print(f\"\\nCell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "print(f\"Probability of effectivelty performing action {env.actions[0]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[0], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[1]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[1], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[2]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[2], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[3]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[3], state+1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f146690",
   "metadata": {},
   "source": [
    "#### Q1. Find an optimal solution to this problem by using the approach that you think is most appropriate. Motivate your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dadd6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'AquaticEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "def acquaticValueIteration(env, maxIterazione = 500, discount = 0.9, maxDifferenza = 1e-3):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: ambiente\n",
    "        maxIterazione: numero massimo di iterazioni\n",
    "        discount: \n",
    "        maxDifferenza: limite superiore da confrontare con delta\n",
    "    \"\"\"\n",
    "\n",
    "    U_1 = [0 for _ in range(env.observation_space.n)] #array contenente i valori degli stati aggiornati\n",
    "    U = U_1.copy()  #array contenente i valori dei vecchi stati\n",
    "    cont = 0\n",
    "\n",
    "    while True:\n",
    "        cont += 1\n",
    "        delta = 0\n",
    "        U = U_1.copy()\n",
    "\n",
    "        for state in range(0, env.observation_space.n):\n",
    "            if env.grid[state] == \"G\":\n",
    "                U_1[state] = env.RS[state]\n",
    "            else:\n",
    "                insiemePunteggiAzioni = [U[state] for _ in range(0, env.action_space.n)]\n",
    "                \n",
    "                for action in range(0, env.action_space.n):\n",
    "                    u_somma = 0\n",
    "                    \n",
    "                    for nextState in range (0, env.observation_space.n):\n",
    "                        u_somma += env.T[state, action, nextState] * U[nextState]\n",
    "                    \n",
    "                    insiemePunteggiAzioni.append(u_somma)\n",
    "\n",
    "                U_1[state] = env.RS[state] + discount * max(insiemePunteggiAzioni)\n",
    "            \n",
    "            delta = max(delta, abs(U_1[state] - U[state]))\n",
    "        \n",
    "        if delta < maxDifferenza * (1 - discount) / discount or cont >= maxIterazione:\n",
    "            break\n",
    "\n",
    "    return(values_to_policy(np.asarray(U), env), U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a45fd6",
   "metadata": {},
   "source": [
    "You can visualize and check your solution using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf89b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metodo: Value Iteration\n",
      "[['D' 'D' 'D' 'D' 'D' 'D' 'R' 'D' 'L' 'L']\n",
      " ['D' 'D' 'D' 'D' 'D' 'R' 'R' 'D' 'L' 'L']\n",
      " ['D' 'D' 'R' 'R' 'D' 'L' 'R' 'D' 'L' 'L']\n",
      " ['D' 'D' 'R' 'R' 'L' 'L' 'R' 'D' 'L' 'L']\n",
      " ['D' 'D' 'D' 'R' 'U' 'D' 'R' 'D' 'L' 'L']\n",
      " ['R' 'D' 'L' 'R' 'R' 'R' 'R' 'D' 'L' 'L']\n",
      " ['R' 'U' 'R' 'R' 'R' 'D' 'D' 'D' 'D' 'D']\n",
      " ['D' 'D' 'D' 'D' 'R' 'R' 'D' 'D' 'D' 'D']\n",
      " ['R' 'R' 'R' 'D' 'D' 'D' 'D' 'D' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'L']]\n",
      "Ricompensa di ogni stato:  (9, 7)  (tipo di stato:  G ) :  20.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Metodo: Value Iteration\")\n",
    "solution, _ = acquaticValueIteration(env)\n",
    "\n",
    "#create a random policy just for testing\n",
    "#solution = np.random.choice(list(env.actions.keys()), env.observation_space.n)\n",
    "\n",
    "\n",
    "visual_solution = np.vectorize(env.actions.get)(solution.reshape(env.rows, env.cols)) \n",
    "\n",
    "print(visual_solution)\n",
    "\n",
    "for state in range(0, env.observation_space.n):\n",
    "    if env.grid[state] == \"G\":\n",
    "        print(\"\\nRicompensa di ogni stato: \", env.state_to_pos(state), \" (tipo di stato: \", env.grid[state], \") : \", env.RS[state])\n",
    "#plot_policy(policy=visual_solution, name_env=\"ex2_render\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e100a",
   "metadata": {},
   "source": [
    "#### Analyze the solution returned by your approach and comment on whether the solution passes by at least two charging stations to reach the coral for every possible execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc75a66a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
